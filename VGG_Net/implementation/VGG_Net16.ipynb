{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zyxNWSeRd1Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "heFDBRgwd1n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL0hY6rbZeLo"
      },
      "source": [
        "<h1 style=\"font-size: 120%; text-align: center;\">VGG16 Architecture (From Scratch)</h1>\n",
        "\n",
        "<p style=\"font-size: 100%;\">\n",
        "VGG16 is a deep convolutional neural network composed of 13 convolution layers\n",
        "followed by 3 fully connected layers. What makes VGG16 special is its simplicity:\n",
        "it uses only <b>3×3 convolution filters</b> and <b>2×2 max-pooling</b> throughout the network.\n",
        "This results in a clean, uniform architecture that is easy to understand and implement.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJVLZBcgVZYx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import ImageFile\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8zUTbfmY4yB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5c70ed-0837-4411-a724-70213f698784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div style=\"\n",
        "  max-width:830px;\n",
        "  margin:22px auto;\n",
        "  padding:22px 28px;\n",
        "  background:#ffffff;\n",
        "  border-radius:18px;\n",
        "  box-shadow:0 12px 30px rgba(0,0,0,0.08);\n",
        "  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial;\n",
        "  line-height:1.55;\n",
        "\">\n",
        "\n",
        "  <h1 style=\"text-align:center; font-size:175%; margin-top:0; color:#111827;\">\n",
        "     Cats vs Dogs Image Dataset\n",
        "  </h1>\n",
        "\n",
        "  <p style=\"font-size:15px; color:#4b5563;\">\n",
        "    This dataset contains a curated collection of <strong>cat and dog images</strong>, designed for binary image\n",
        "    classification tasks. It is commonly used in computer vision to build and evaluate convolutional neural networks,\n",
        "    understand transfer learning, and experiment with image preprocessing and augmentation.  \n",
        "  </p>\n",
        "\n",
        "  <hr style=\"border:none; height:1px; background:#e5e7eb; margin:20px 0;\">\n",
        "\n",
        "  <h2 style=\"font-size:140%; color:#1f2937; margin-bottom:8px;\"> Dataset Overview</h2>\n",
        "  <ul style=\"font-size:15px; color:#374151; margin-left:22px; margin-top:6px;\">\n",
        "    <li>Two primary classes: <strong>Cats</strong> and <strong>Dogs</strong>.</li>\n",
        "    <li>Images vary in pose, lighting, background, and resolution.</li>\n",
        "    <li>Ideal for training deep models like VGG16, ResNet, MobileNet, etc.</li>\n",
        "    <li>Often used to demonstrate transfer learning and convolutional feature extraction.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h2 style=\"font-size:140%; color:#1f2937; margin-bottom:8px; margin-top:20px;\"> Why This Dataset?</h2>\n",
        "  <ul style=\"font-size:15px; color:#374151; margin-left:22px; margin-top:6px;\">\n",
        "    <li><strong>Perfect for beginners</strong>  simple binary classification problem.</li>\n",
        "    <li><strong>Great for transfer learning</strong>  VGG16, ResNet, EfficientNet, etc. perform very well.</li>\n",
        "    <li><strong>Large enough</strong> to prevent overfitting but small enough for fast training on GPUs.</li>\n",
        "    <li><strong>Diverse images</strong> make it suitable for real-world generalization.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h2 style=\"font-size:140%; color:#1f2937; margin-bottom:8px; margin-top:20px;\"> Typical Use Cases</h2>\n",
        "  <ul style=\"font-size:15px; color:#374151; margin-left:22px; margin-top:6px;\">\n",
        "    <li>Transfer Learning demonstrations</li>\n",
        "    <li>Training CNNs from scratch</li>\n",
        "    <li>Experimenting with data augmentation techniques</li>\n",
        "    <li>Evaluating classification accuracy & model generalization</li>\n",
        "  </ul>\n",
        "\n",
        "  <div style=\"margin-top:20px; padding:12px 16px; border-left:4px solid #10b981; background:#ecfdf5; color:#065f46;\">\n",
        "    <strong>Note:</strong> This dataset is used solely for educational and research purposes.\n",
        "  </div>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "kWNqkJgfp8t1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> Dataset File Structure</h2>\n",
        "\n",
        "\n",
        "\n",
        "<pre>\n",
        "data/\n",
        "   train/\n",
        "      cats/\n",
        "      dogs/\n",
        "   val/\n",
        "      cats/\n",
        "      dogs/\n",
        "</pre>\n"
      ],
      "metadata": {
        "id": "f3F4f-zfazg2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7y6yXgCOO-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875dfd04-eb8a-4e6b-b93f-818680baff63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cat_dog_data\n",
            "test_set  training_set\n",
            "cats  dogs\n",
            "cats  dogs\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/DATASETZ\"\n",
        "!ls \"/content/drive/My Drive/DATASETZ/cat_dog_data\"\n",
        "!ls \"/content/drive/My Drive/DATASETZ/cat_dog_data/training_set\"\n",
        "!ls \"/content/drive/My Drive/DATASETZ/cat_dog_data/test_set\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dir = \"/content/drive/MyDrive/DATASETZ/cat_dog_data/training_set\"\n",
        "val_dir   = \"/content/drive/MyDrive/DATASETZ/cat_dog_data/test_set\"\n",
        "\n",
        "\n",
        "# Verify contents\n",
        "import os\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive/DATASETZ/cat_dog_data\"):\n",
        "    if \".ipynb_checkpoints\" not in root:\n",
        "        print(root, \"->\", len(files), \"files\")"
      ],
      "metadata": {
        "id": "GR7eOh_bX5c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10aa329b-539c-41a4-c3e0-3014cb1b717b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DATASETZ/cat_dog_data -> 0 files\n",
            "/content/drive/MyDrive/DATASETZ/cat_dog_data/test_set -> 0 files\n",
            "/content/drive/MyDrive/DATASETZ/cat_dog_data/test_set/cats -> 991 files\n",
            "/content/drive/MyDrive/DATASETZ/cat_dog_data/test_set/dogs -> 1000 files\n",
            "/content/drive/MyDrive/DATASETZ/cat_dog_data/training_set -> 0 files\n",
            "/content/drive/MyDrive/DATASETZ/cat_dog_data/training_set/cats -> 4000 files\n",
            "/content/drive/MyDrive/DATASETZ/cat_dog_data/training_set/dogs -> 4000 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(train_dir))\n",
        "print(os.path.exists(val_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3u5zzDLQV7n",
        "outputId": "0dad93f8-7a89-4a57-c4ea-ef7642a696c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> Transformations / Preprocessing</h2>\n",
        "\n",
        "<p style=\"font-size: 95%;\">\n",
        "Since we are training <b>from scratch</b>, we keep preprocessing minimal—resize images, apply augmentation,\n",
        "and convert to tensors. No ImageNet normalization is used because we are not using pretrained weights.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "fIyEeWbeZ4yJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj1Qiya5Y6Ga"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TRANSFORMS\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
        "    ], p=0.3),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> Loading the Dataset</h2>\n",
        "\n",
        "<p style=\"font-size: 95%;\">\n",
        "PyTorch's <code>ImageFolder</code> automatically reads images from the folder structure.\n",
        "We create DataLoaders to feed images into the model in batches.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "n5oNnFMSbL_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwc0mUrTZCzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1826a51b-bf89-4294-8520-ab21923d61be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['cats', 'dogs']\n",
            "Train images: 8000\n",
            "Val images: 1991\n"
          ]
        }
      ],
      "source": [
        "# DATASETS & DATALOADERS\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset   = ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=0)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                        shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "print(\"Classes:\", train_dataset.classes)\n",
        "print(\"Train images:\", len(train_dataset))\n",
        "print(\"Val images:\", len(val_dataset))\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> VGG16 Architecture (Full From-Scratch Model)</h2>\n",
        "\n",
        "<p style=\"font-size: 95%;\">\n",
        "Below is the complete VGG16 architecture defined manually, not using any pretrained weights.\n",
        "It includes:\n",
        "<ul>\n",
        "<li>5 Convolutional Blocks</li>\n",
        "<li>Each block contains 2 or 3 Conv layers</li>\n",
        "<li>Max-pooling after each block</li>\n",
        "<li>Fully connected classifier at the end</li>\n",
        "</ul>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "BeadzYMQbPfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D61KUhW0ZDR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae20916-9985-4a14-dd32-f1622fbed8bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): Dropout(p=0.5, inplace=False)\n",
            "    (9): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Model device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGG16, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "           nn.Flatten(),\n",
        "           nn.Linear(512 * 7 * 7, 4096),\n",
        "           nn.ReLU(True),\n",
        "           nn.BatchNorm1d(4096),\n",
        "           nn.Dropout(0.5),\n",
        "\n",
        "           nn.Linear(4096, 4096),\n",
        "           nn.ReLU(True),\n",
        "           nn.BatchNorm1d(4096),\n",
        "           nn.Dropout(0.5),\n",
        "\n",
        "           nn.Linear(4096, num_classes)\n",
        "         )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "device = torch.device(\"cuda\")\n",
        "model = VGG16(num_classes).to(device)\n",
        "print(model)\n",
        "# Selective unfreeze: Train only Block 5\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False  # freeze all feature extractor\n",
        "\n",
        "for idx in range(24, 31):        # Block 5 layers\n",
        "    for param in model.features[idx].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "print(\"Model device:\", next(model.parameters()).device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = next(iter(train_loader))\n",
        "inputs = inputs.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "print(\"Batch device:\", inputs.device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBlh1JSfQ37P",
        "outputId": "a6b1441e-bef7-407f-8b84-0dab59b929fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> Loss Function and Optimizer</h2>\n",
        "\n",
        "<p style=\"font-size: 95%;\">\n",
        "We use CrossEntropyLoss for classification and Adam optimizer for stable training.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "5mywAYYDbxvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# IMPORTANT: Create optimizer ONLY for trainable parameters (classifier)\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                        lr=1e-4, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "PKpY0pA3bwz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> LR Scheduler</h2>"
      ],
      "metadata": {
        "id": "wOHQimBlUMjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n"
      ],
      "metadata": {
        "id": "2_ChlMOTUGxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> Training Loop</h2>\n",
        "\n",
        "<p style=\"font-size: 95%;\">\n",
        "This function handles:\n",
        "<ul>\n",
        "<li>Forward pass</li>\n",
        "<li>Backpropagation</li>\n",
        "<li>Loss & accuracy calculation</li>\n",
        "<li>Tracking the best model weights</li>\n",
        "</ul>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "EfABsr3gbp-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orGPr6p9ZK9J"
      },
      "outputs": [],
      "source": [
        "# TRAINING LOOP\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, epochs=20):\n",
        "    best_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            # train mode enables dropout & grads; eval freezes them\n",
        "            model.train() if phase == \"train\" else model.eval()\n",
        "\n",
        "            dataloader = train_loader if phase == \"train\" else val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc  = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "            print(f\"{phase.upper()} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            # Store metrics for plotting\n",
        "            if phase == \"train\":\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accs.append(epoch_acc.item())\n",
        "            else:\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accs.append(epoch_acc.item())\n",
        "\n",
        "            #  SAVE BEST MODEL CHECKPOINT\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'model_state_dict': best_wts,\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'best_acc': best_acc\n",
        "                }, \"vgg16_best_checkpoint.pth\")\n",
        "\n",
        "                print(f\"Checkpoint saved! New best accuracy: {best_acc:.4f}\")\n",
        "\n",
        "\n",
        "        # Step scheduler AFTER each epoch\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\nBest Validation Accuracy: {best_acc:.4f}\")\n",
        "    model.load_state_dict(best_wts)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def estimate_training_time(model, loader, epochs=35):\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    inputs, labels = next(iter(loader))\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    _ = model(inputs)\n",
        "\n",
        "    # Measure time for 10 batches\n",
        "    start = time.time()\n",
        "    batches_to_test = 10\n",
        "    count = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)\n",
        "\n",
        "        count += 1\n",
        "        if count >= batches_to_test:\n",
        "            break\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    sec_per_batch = (end - start) / batches_to_test\n",
        "    batches_per_epoch = len(loader)\n",
        "    epoch_time = sec_per_batch * batches_per_epoch\n",
        "    total_time = epoch_time * epochs\n",
        "\n",
        "    print(f\"\\nEstimated time per batch: {sec_per_batch:.4f} sec\")\n",
        "    print(f\"Estimated time per epoch: {epoch_time/60:.2f} minutes\")\n",
        "    print(f\"Estimated total time ({epochs} epochs): {total_time/60:.2f} minutes\")\n",
        "\n",
        "\n",
        "estimate_training_time(model, train_loader, epochs=35)\n"
      ],
      "metadata": {
        "id": "Op7EzhYpNt4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06008530-a19e-4342-d838-c56ce79dd781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estimated time per batch: 3.4153 sec\n",
            "Estimated time per epoch: 14.23 minutes\n",
            "Estimated total time (35 epochs): 498.06 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> Start Training</h2>\n"
      ],
      "metadata": {
        "id": "K9Wa-CRGb6g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  TRAIN MODEL\n",
        "\n",
        "trained_model = train_model(model, criterion, optimizer, scheduler, epochs=35)\n",
        "\n"
      ],
      "metadata": {
        "id": "L1Esy8phZs-7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "bc731d82-bf39-40d2-dc5b-736470cd5c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1903915350.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  TRAIN MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  save\n",
        "torch.save(trained_model.cpu().state_dict(), \"vgg16_from_scratch.pth\")\n",
        "trained_model.to(device)\n",
        "\n",
        "print(\"\\nSaved model as vgg16_from_scratch.pth\")\n"
      ],
      "metadata": {
        "id": "R_W00MkwZwTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> Prediction Function</h2>\n",
        "\n",
        "<p style=\"font-size: 95%;\">\n",
        "A simple helper function to classify new images using the trained model.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "R1lh6viibhwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(path, model):\n",
        "    model.eval()   # <-- always ensure evaluation mode\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img = val_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "\n",
        "    # Return class name\n",
        "    class_idx = pred.item()\n",
        "    return train_dataset.classes[class_idx]\n"
      ],
      "metadata": {
        "id": "8MATmlVwZ07v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\"> ACCURACY & LOSS CURVES</h2>\n"
      ],
      "metadata": {
        "id": "qidXOY2MVTGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------- LOSS PLOT --------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------- ACCURACY PLOT --------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_accs, label=\"Train Acc\")\n",
        "plt.plot(val_accs, label=\"Val Acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AR5JgCixV-Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 115%;\">CONFUSION MATRIX</h2>\n"
      ],
      "metadata": {
        "id": "M23KuoXyWC_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def evaluate_confusion_matrix(model, dataloader):\n",
        "    model.eval()\n",
        "    preds_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            preds_list.extend(preds.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(labels_list, preds_list)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=['Cat', 'Dog'])\n",
        "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# -------------------- CALL IT --------------------\n",
        "evaluate_confusion_matrix(model, val_loader)\n"
      ],
      "metadata": {
        "id": "p6GeYg0tWFU6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}